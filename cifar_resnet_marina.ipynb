{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" #<- for the common server(SSH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "master_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from typing import List\n",
    "# import lovely_tensors as lt\n",
    "# lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for testing purpose, remove later!!!!\n",
    "trainset = Subset(trainset, list(range(5000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_model_params(model):\n",
    "    return  sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "base_model.fc = nn.Linear(512, 10)\n",
    "base_loss = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11181642"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_model_params(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 5\n",
    "worker_dataset_size = len(trainset) // num_workers\n",
    "batch_size = 64\n",
    "batch_size_worker_dataset = 256 #\n",
    "K = 10**6\n",
    "gamma = 1e-3\n",
    "VR_used = True\n",
    "TRAIN_SET_SIZE = len(trainset)\n",
    "\n",
    "# W = D / K - 1 (D - num of params)\n",
    "W = count_model_params(base_model) / K - 1\n",
    "\n",
    "if VR_used:\n",
    "    p = min(\n",
    "        1.0 / (1 + W), batch_size_worker_dataset / (batch_size_worker_dataset + worker_dataset_size)\n",
    "    )  # TODO reimplemented, may be incorrect\n",
    "else:\n",
    "    p = 1.0/(1+W)\n",
    "\n",
    "TEST_LEN = 500\n",
    "rand_list = np.random.randint(low=0, high=len(trainset), size=TEST_LEN)\n",
    "testloader = DataLoader(Subset(trainset, rand_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkerModel:\n",
    "    def __init__(\n",
    "        self, model_name, dataset, VR=False, batch_size_worker_dataset=256\n",
    "    ) -> None:\n",
    "        if model_name == \"ResNet18\":\n",
    "            self.model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "            self.model.fc = nn.Linear(512, 10)\n",
    "        else:\n",
    "            raise NotImplementedError(\"This model is not implemented\")\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.batch_size_worker_dataset = batch_size_worker_dataset\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.VR = VR\n",
    "        self.D = count_model_params(self.model)\n",
    "        self.K = K\n",
    "\n",
    "    def full_grad(self):\n",
    "        self.mean_by_trainset = torch.tensor(1 / TRAIN_SET_SIZE).to(self.device)\n",
    "\n",
    "        self.model.train()\n",
    "        for batch in tqdm(self.dataloader):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.mean_by_trainset * F.cross_entropy(outputs, labels, reduction='sum')\n",
    "            loss.backward()\n",
    "\n",
    "        result = []\n",
    "        for param in self.model.parameters():\n",
    "            result.append(param.grad.data.detach().clone())\n",
    "            param.grad = None\n",
    "        return result\n",
    "\n",
    "    def substarct_grads(self, g_prev, g_next):\n",
    "        g_result = []\n",
    "        assert len(g_prev) == len(g_next)\n",
    "        for i in range(len(g_prev)):\n",
    "            g_result.append(g_next[i] - g_prev[i])\n",
    "        return g_result\n",
    "\n",
    "    def add_grads(self, grad1, grad2):\n",
    "        g_result = []\n",
    "        assert len(grad1) == len(grad2)\n",
    "        for i in range(len(grad1)):\n",
    "            g_result.append(grad1[i] + grad2[i])\n",
    "        return g_result\n",
    "\n",
    "    def compress_vector(self, v):\n",
    "        \"\"\"Vector compression with RandK algo\"\"\"\n",
    "        S = torch.randperm(self.D)\n",
    "        S = S[0 : self.K]\n",
    "        out = torch.zeros_like(v)\n",
    "        out[S] = self.D / self.K * v[S]\n",
    "        return out\n",
    "\n",
    "    def MARINA_step(self, gk, ck):\n",
    "        self.mean_by_batch = (\n",
    "            torch.Tensor([1.0 / (self.batch_size_worker_dataset)]).to(self.device)\n",
    "            if self.VR\n",
    "            else self.mean_by_trainset\n",
    "        )\n",
    "        self.mean_by_trainset = torch.tensor(1 / TRAIN_SET_SIZE).to(self.device)\n",
    "\n",
    "        if ck == 1:\n",
    "            k = 0\n",
    "            for p in self.model.parameters():\n",
    "                p.data = p.data - gamma * gk[k]\n",
    "                k = k + 1\n",
    "\n",
    "            self.model.train()\n",
    "            for batch in self.dataloader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.mean_by_trainset * F.cross_entropy(outputs, labels, reduction='sum')\n",
    "                loss.backward()\n",
    "\n",
    "            result = []\n",
    "            for param in self.model.parameters():\n",
    "                result.append(param.grad.data.detach().clone())\n",
    "                param.grad = None\n",
    "            return result\n",
    "\n",
    "        elif ck == 0:\n",
    "            if self.VR:\n",
    "                indicies = torch.randperm(len(self.dataset))[\n",
    "                    0 : self.batch_size_worker_dataset\n",
    "                ]\n",
    "                subset = torch.utils.data.Subset(self.dataset, indicies)\n",
    "            else:\n",
    "                subset = self.dataset\n",
    "\n",
    "            minibatch_loader = DataLoader(subset, batch_size=batch_size)\n",
    "\n",
    "            self.model.train()\n",
    "            for batch in minibatch_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.mean_by_batch * F.cross_entropy(outputs, labels, reduction='sum')\n",
    "                loss.backward()\n",
    "\n",
    "            g_batch_prev = []\n",
    "            for p in self.model.parameters():\n",
    "                g_batch_prev.append(p.grad.data.detach().clone())\n",
    "                p.grad = None\n",
    "\n",
    "            k = 0\n",
    "            for p in self.model.parameters():\n",
    "                p.data = p.data - gamma * gk[k]\n",
    "                k = k + 1\n",
    "\n",
    "            for batch in minibatch_loader:\n",
    "                inputs, labels = batch\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.mean_by_batch * F.cross_entropy(outputs, labels, reduction='sum')\n",
    "                loss.backward()\n",
    "\n",
    "            g_batch_next = []\n",
    "            for p in self.model.parameters():\n",
    "                g_batch_next.append(p.grad.data.detach().clone())\n",
    "                p.grad = None\n",
    "\n",
    "            g_delta = self.substarct_grads(g_batch_prev, g_batch_next)\n",
    "\n",
    "            # ===============================================================================================\n",
    "            \"\"\"\n",
    "            Convert gradient defference to the flatten form of 1D vector\n",
    "            To compress it with RandK compressor and the turn it back to the initial form\n",
    "            \"\"\"\n",
    "            delta_offset = 0\n",
    "            g_delta_flatten = torch.zeros(self.D).to(self.device)\n",
    "\n",
    "            for t in range(len(g_delta)):\n",
    "                offset = len(g_delta[t].flatten(0))\n",
    "                g_delta_flatten[(delta_offset) : (delta_offset + offset)] = g_delta[\n",
    "                    t\n",
    "                ].flatten(0)\n",
    "                delta_offset += offset\n",
    "\n",
    "            g_delta_flatten = self.compress_vector(g_delta_flatten)\n",
    "\n",
    "            delta_offset = 0\n",
    "            for t in range(len(g_delta)):\n",
    "                offset = len(g_delta[t].flatten(0))\n",
    "                g_delta[t].flatten(0)[:] = g_delta_flatten[\n",
    "                    (delta_offset) : (delta_offset + offset)\n",
    "                ]\n",
    "                delta_offset += offset\n",
    "            # ===============================================================================================\n",
    "\n",
    "            g_resulting = self.add_grads(gk, g_delta)\n",
    "            return g_resulting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for i in range(num_workers):\n",
    "    start_idx = i * worker_dataset_size\n",
    "    end_idx = (i + 1) * worker_dataset_size\n",
    "\n",
    "    # Create a Subset of the dataset for the current part\n",
    "    subset = Subset(trainset, list(range(start_idx, end_idx)))\n",
    "    datasets.append(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_full_grad(workers):\n",
    "    \"\"\"\n",
    "    Initial step of the MARINA Algorithm\n",
    "    calculate full gradient on all the workers\n",
    "    to get the first value of g\n",
    "    \"\"\"\n",
    "    workers_full_grad = []\n",
    "    for worker in workers:\n",
    "        workers_full_grad.append(worker.full_grad()) # calculate on all the workers\n",
    "    g0 = workers_full_grad[0]\n",
    "    for i in range(1, num_workers): # summing up all the gradients from the client of the master node\n",
    "        for j in range(len(workers_full_grad[i])):\n",
    "            g0[j] = g0[j] + workers_full_grad[i][j].to(master_device)\n",
    "    gk = list(map(lambda x: 1/num_workers * x, g0)) # take mean of g after summing up from all workers\n",
    "    return gk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_MARINA_step(workers: List[WorkerModel], gk):\n",
    "    step_grads = []\n",
    "    for worker in workers:\n",
    "        rand_num = np.random.random()\n",
    "        ck = 1 if rand_num < p else 0\n",
    "\n",
    "        step_grads.append(worker.MARINA_step(gk, ck))\n",
    "\n",
    "    total_grad = step_grads[0]\n",
    "    for i in range(1, len(step_grads)):\n",
    "        for j in range(len(step_grads[i])):\n",
    "            total_grad[j] = total_grad[j] + step_grads[i][j].to(master_device)\n",
    "\n",
    "    total_grad = list(map(lambda x: 1/num_workers * x, total_grad))\n",
    "    return total_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(model, dataloader, device):\n",
    "    avg_accuracy = 0\n",
    "\n",
    "    model.eval()\n",
    "    for inputs, outputs in dataloader:\n",
    "        inputs, outputs = inputs.to(device), outputs.to(device)                  # move to device\n",
    "        logits = model(inputs)                                                   # forward-pass: Make a forward pass through the network\n",
    "        avg_accuracy += (logits.data.argmax(1) == outputs).sum().item()\n",
    "\n",
    "    avg_accuracy /= TEST_LEN\n",
    "    model.train()\n",
    "\n",
    "    return avg_accuracy\n",
    "\n",
    "def getLossAndGradNorm(model, dataloader, device):\n",
    "    total_loss = 0\n",
    "    grad_norm = 0\n",
    "    one_inv_samples = torch.Tensor([1.0/TEST_LEN]).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    for inputs, outputs in dataloader:\n",
    "        inputs, outputs = inputs.to(device), outputs.to(device)                             # move to device\n",
    "\n",
    "        logits = model(inputs)\n",
    "\n",
    "        loss = one_inv_samples * F.cross_entropy(logits, outputs, reduction='sum')          # compute objective\n",
    "        loss.backward()                                                                     # compute the gradient (backward-pass)\n",
    "        total_loss += loss\n",
    "\n",
    "    for p in model.parameters():\n",
    "        grad_norm += torch.norm(p.grad.data.flatten(0))**2\n",
    "        p.grad = None\n",
    "\n",
    "    model.train()\n",
    "    return total_loss, grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, trainloader):\n",
    "    acc = getAccuracy(model, trainloader, device=master_device)\n",
    "    loss, grad_norm = getLossAndGradNorm(model, trainloader, device=master_device)\n",
    "    return acc, loss, grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "losses = []\n",
    "grad_norms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='MARINA-1000Iters.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(MAX_ITER):\n",
    "    workers = []\n",
    "    for i in range(num_workers):\n",
    "        curr_worker = WorkerModel('ResNet18', datasets[i], VR=VR_used)\n",
    "        workers.append(curr_worker)\n",
    "        print(f\"Worker {i+1} on device - {curr_worker.device}\")\n",
    "    print(\"==================Taking full gradient as first step==================\")\n",
    "    gk = take_full_grad(workers)\n",
    "    print('========================Starting MARINA steps=========================')\n",
    "    for i in tqdm(range(MAX_ITER-1)):\n",
    "        if i%10 == 0:\n",
    "            acc, loss, g_norm = eval_model(workers[0].model, testloader)\n",
    "            accs.append(acc)\n",
    "            losses.append(loss.item())\n",
    "            grad_norms.append(g_norm.item())\n",
    "            print(f\"Step={i+1}, accuracy={acc}, loss={loss.item()}, gradient_norm={g_norm}\")\n",
    "            logging.info(f\"Step={i+1}, accuracy={acc}, loss={loss.item()}, gradient_norm={g_norm}\")\n",
    "            # accs_np = np.array(accs)\n",
    "            # losses_np = np.array(losses)\n",
    "            # grad_norms_np = np.array(grad_norms)\n",
    "            # np.save('no_vr_accs.npy', accs_np)\n",
    "            # np.save('no_vr_losses.npy', losses_np)\n",
    "            # np.save('no_vr_grad_norms.npy', grad_norms_np)\n",
    "        gk_next = take_MARINA_step(workers, gk)\n",
    "        gk = gk_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 1 on device - cuda\n",
      "Worker 2 on device - cuda\n",
      "Worker 3 on device - cuda\n",
      "Worker 4 on device - cuda\n",
      "Worker 5 on device - cuda\n",
      "==================Taking full gradient as first step==================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a84a77b5e2a44eb9dc912c41cdde272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bc5b2313d44d85a2a7bf5121d42e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8a4da609c84863a442bed7964b7129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0745c4ece047a8a6da64955185ffb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c41b366c5e47e99e57b5d640c8da8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================Starting MARINA steps=========================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae923243449c4e7790b854b469d97470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=1, accuracy=0.094, loss=2.4187819957733154, gradient_norm=13.732355117797852\n",
      "Step=11, accuracy=0.106, loss=2.6338002681732178, gradient_norm=123.46015167236328\n",
      "Step=21, accuracy=0.108, loss=2.6461358070373535, gradient_norm=110.2783203125\n",
      "Step=31, accuracy=0.106, loss=2.6153135299682617, gradient_norm=117.86164093017578\n",
      "Step=41, accuracy=0.112, loss=2.6055421829223633, gradient_norm=123.98627471923828\n",
      "Step=51, accuracy=0.106, loss=2.6096091270446777, gradient_norm=123.91548156738281\n",
      "Step=61, accuracy=0.116, loss=2.605438232421875, gradient_norm=119.4603271484375\n",
      "Step=71, accuracy=0.114, loss=2.570636749267578, gradient_norm=98.48695373535156\n",
      "Step=81, accuracy=0.108, loss=2.588923692703247, gradient_norm=111.5507583618164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m MAX_ITER \u001b[39m=\u001b[39m \u001b[39m2500\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m train_loop(MAX_ITER)\n",
      "\u001b[1;32m/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStep=\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, accuracy=\u001b[39m\u001b[39m{\u001b[39;00macc\u001b[39m}\u001b[39;00m\u001b[39m, loss=\u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m, gradient_norm=\u001b[39m\u001b[39m{\u001b[39;00mg_norm\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# accs_np = np.array(accs)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# losses_np = np.array(losses)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# grad_norms_np = np.array(grad_norms)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# np.save('no_vr_accs.npy', accs_np)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# np.save('no_vr_losses.npy', losses_np)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# np.save('no_vr_grad_norms.npy', grad_norms_np)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m gk_next \u001b[39m=\u001b[39m take_MARINA_step(workers, gk)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m gk \u001b[39m=\u001b[39m gk_next\n",
      "\u001b[1;32m/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb Cell 18\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     rand_num \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandom()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     ck \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m rand_num \u001b[39m<\u001b[39m p \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     step_grads\u001b[39m.\u001b[39mappend(worker\u001b[39m.\u001b[39;49mMARINA_step(gk, ck))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m total_grad \u001b[39m=\u001b[39m step_grads[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(step_grads)):\n",
      "\u001b[1;32m/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=139'>140</a>\u001b[0m     g_delta_flatten[(delta_offset) : (delta_offset \u001b[39m+\u001b[39m offset)] \u001b[39m=\u001b[39m g_delta[\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=140'>141</a>\u001b[0m         t\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=141'>142</a>\u001b[0m     ]\u001b[39m.\u001b[39mflatten(\u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m     delta_offset \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m offset\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m g_delta_flatten \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompress_vector(g_delta_flatten)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m delta_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(g_delta)):\n",
      "\u001b[1;32m/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompress_vector\u001b[39m(\u001b[39mself\u001b[39m, v):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Vector compression with RandK algo\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     S \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrandperm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mD)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     S \u001b[39m=\u001b[39m S[\u001b[39m0\u001b[39m : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mK]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(v)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_ITER = 2500\n",
    "train_loop(MAX_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (50,) and (9,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb Cell 21\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m fig\u001b[39m.\u001b[39msuptitle(\u001b[39m'\u001b[39m\u001b[39mMARINA convergence results\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m2500\u001b[39m, step\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m ax[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mplot(X, accs)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ax[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mset_xlabel(\u001b[39m'\u001b[39m\u001b[39mStep\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.100.30.241/home/user/commits/commit_messages_generation/opts/cifar_resnet_marina.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m ax[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mset_ylabel(\u001b[39m'\u001b[39m\u001b[39mAccuaracy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/commit-generation/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/miniconda3/envs/commit-generation/lib/python3.10/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/miniconda3/envs/commit-generation/lib/python3.10/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (50,) and (9,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2100x700 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 3,  figsize=(21, 7))\n",
    "fig.suptitle('MARINA convergence results')\n",
    "X = np.arange(2500, step=50)\n",
    "\n",
    "ax[0].plot(X, accs)\n",
    "ax[0].set_xlabel('Step')\n",
    "ax[0].set_ylabel('Accuaracy')\n",
    "\n",
    "ax[1].plot(X, losses)\n",
    "ax[1].set_xlabel('Step')\n",
    "ax[1].set_ylabel('Loss')\n",
    "\n",
    "ax[2].plot(X, grad_norms)\n",
    "ax[2].set_ylabel(\"$\\| \\\\nabla f(x)\\|^2$\")\n",
    "ax[2].set_xlabel('Step')\n",
    "\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "ax[2].grid()\n",
    "plt.savefig('marina_convergence.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
